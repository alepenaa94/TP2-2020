{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/alepenaa94/TP1_Real_or_Not/blob/master/TP1_Real_or_Not.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iwY7zLPkMCLI"
   },
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook vamos a realizar todo el procesamiento del set de datos, limpieza en los campos de ser necesario y la generacion de los features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "0R_dMyhfL8QX",
    "outputId": "5d011d54-140f-4716-cdd7-7b31c412e540"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KiQERldrjl68"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../Data/train.csv', encoding='latin-1',dtype={'id': np.uint16,'target': np.bool})\n",
    "test_df = pd.read_csv('../Data/test.csv', encoding='latin-1',dtype={'id': np.uint16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "sTfW9uPBjl10",
    "outputId": "237774d0-8fb8-49be-a7a2-ab000ed1c736",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0    True  \n",
       "1    True  \n",
       "2    True  \n",
       "3    True  \n",
       "4    True  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.concat([train_df,test_df])\n",
    "tweets = tweets[['id','keyword','location','text','target']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza de la información"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En base a lo aprendido en el TP1, limpiamos la información de algunos campos del dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['keyword'] = tweets['keyword'].str.replace('%20',' ')\n",
    "tweets['keyword'].fillna('None',inplace=True)\n",
    "tweets['location'].fillna('Unknown',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volvemos a generar las columnas utilizadas para el TP1 ya que podrían resultar features útiles para el modelo de predicción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Cantidad de palabras en el tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['cantidad_de_palabras'] = tweets['text'].str.count(' ') + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Longitud del tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['longitud_del_tweet'] = tweets['text'].str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Cuartiles de longitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>longitud_del_tweet</td>\n",
       "      <td>10876.0</td>\n",
       "      <td>101.790916</td>\n",
       "      <td>34.141486</td>\n",
       "      <td>5.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>169.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      count        mean        std  min   25%    50%    75%  \\\n",
       "longitud_del_tweet  10876.0  101.790916  34.141486  5.0  78.0  108.0  134.0   \n",
       "\n",
       "                      max  \n",
       "longitud_del_tweet  169.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['longitud_del_tweet'].describe().to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.loc[tweets['longitud_del_tweet'] < 78.0,'longitud_categ'] = \"0 a 25\"\n",
    "tweets.loc[tweets['longitud_del_tweet'] >= 78.0,'longitud_categ'] = \"25 a 50\"\n",
    "tweets.loc[tweets['longitud_del_tweet'] >= 108.0,'longitud_categ'] = \"50 a 75\"\n",
    "tweets.loc[tweets['longitud_del_tweet'] >= 134.0,'longitud_categ'] = \"75 a 100\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos el método de Smothing para encodear las variables categoricas. (El código está basado en el obtenido de la siguiente fuente: https://gist.github.com/marnixkoops/e68815d30474786e2b293682ed7cdb01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothing(df, column, target, weight=100):\n",
    "    mean = df[target].mean()\n",
    "    agg = df.groupby(column)[target].agg(['count', 'mean'])\n",
    "\n",
    "    dic = {}\n",
    "    \n",
    "    for i in df[column].unique():\n",
    "        dic[i] = (agg.loc[i]['count'] * agg.loc[i]['mean'] + weight * mean) / (agg.loc[i]['count'] + weight)\n",
    "        \n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tweets[:len(train_df)]\n",
    "df['target'] = df['target'].astype('bool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_categ_encoding_dic = smoothing(df,'longitud_categ','target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['longitud_categ'] = tweets['longitud_categ'].map(long_categ_encoding_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Realiza menciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['tiene_menciones'] = tweets['text'].str.contains('@').astype('uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Tweet expresivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['es_expresivo'] = (tweets['text'].str.contains('\\!\\!') | tweets['text'].str.contains('\\?\\?')).astype('uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Cantidad de hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['cantidad_de_hashtags'] = tweets['text'].str.count('#')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Tiene links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['tiene_links'] = tweets['text'].str.contains('http').astype('uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Ubicaciones con tweet único"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def location_unico(location):\n",
    "    ubicaciones_unicas = tweets['location'].value_counts()[tweets['location'].value_counts() == 1].index\n",
    "    return (location in ubicaciones_unicas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['location_unico'] = tweets['location'].map(location_unico).astype('uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Encoding del campo keyword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como la columna keyword es de tipo categórico, vamos a codificarla usando el método de Smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_encoding_dic = smoothing(df,'keyword','target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['keyword_encoded'] = tweets['keyword'].map(keywords_encoding_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\FF633NG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\FF633NG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\FF633NG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#Vamos a limpiar un poco el tweet.\n",
    "pattern_exclude = '(one|dont|cant|would|im|people|go|make|time|love|amp|get|house|update|talk'+\\\n",
    "                  '|want|today|know|say|us|day|crush|see|back|think|look|rigth|remember|car|shes'+\\\n",
    "                  '|thing|let|still|lol|much|thank|take|way|youre|road|another|really|save|hows'+\\\n",
    "                  '|play|even|theres|everyone|feel|year|work|check|two|great|ing|like|sink|href|hr|hs'+\\\n",
    "                  '|every|build|youtuve|video|n|home|body|bag|photo|stay|game|start|gt|fuck|help'+\\\n",
    "                  '|best|well|california|end|live|e|rt|wreck|plan|full|may|ies|u|could|many|last'+\\\n",
    "                  '|find|service|leave|collapse|world|war|destroy|wound|break|right|hear|school)+'\n",
    "\n",
    "def filter_words(tweet):\n",
    "    tweet = re.sub(r'(\\b[\\w]+:\\/\\/[\\w -\\?&;#~=\\.\\/@]+[\\w\\/])', ' ', tweet)\n",
    "    tweet = re.sub(r'\\'', '', tweet)\n",
    "    return re.sub(r'[www.]*[A-z]+.(com|gov|edu|net|mil|org|io|int)+', ' ', tweet)\n",
    "\n",
    "def text_to_blob(tweet):\n",
    "    tweet_blob = TextBlob(str(tweet))\n",
    "    return ' '.join(tweet_blob.words)\n",
    "\n",
    "\n",
    "def normalization(tweet_list):\n",
    "        lem = WordNetLemmatizer()\n",
    "        normalized_tweet = []\n",
    "        for word in tweet_list:\n",
    "            word_aux = word.lower().strip()\n",
    "            if re.match(pattern_exclude,\n",
    "                        word_aux):\n",
    "                continue\n",
    "            normalized_text = lem.lemmatize(word_aux,'v')\n",
    "            normalized_tweet.append(normalized_text)\n",
    "        return normalized_tweet\n",
    "\n",
    "def clean(tweet):\n",
    "    tweet_list = [word for word in (text_to_blob(filter_words(tweet))).split()]\n",
    "    clean_tokens = [tkn for tkn in tweet_list if re.match(r'[A-z]+', tkn)]\n",
    "    clean_s = ' '.join(clean_tokens)\n",
    "    l_aux = normalization(clean_s.split())\n",
    "    return ' '.join([word for word in l_aux if word not in stopwords.words('english')])\n",
    "\n",
    "tweets['clean_text'] = tweets['text'].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = tweets[:len(train_df)]['clean_text'].to_list()\n",
    "\n",
    "tfidf_vectorizer=TfidfVectorizer(analyzer='word', ngram_range=(1,3),sublinear_tf=True, min_df=5, norm='l2',max_features=500, encoding='latin-1', stop_words='english')\n",
    "\n",
    "tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_feature(doc):\n",
    "    #Calcula la similitud entre el doc y lo calculado para todo el set train.\n",
    "    query=TfidfVectorizer(vocabulary=tfidf_vectorizer.vocabulary_)\n",
    "    query=query.fit_transform([doc])\n",
    "    s=pd.Series(linear_kernel(query,tfidf_vectorizer_vectors)[0])\n",
    "    return s.sum()\n",
    "    \n",
    "tweets['tfidf_score']= tweets['clean_text'].apply(similarity_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000000      987\n",
       "57.567868      59\n",
       "77.532286      58\n",
       "106.542617     53\n",
       "78.021807      45\n",
       "             ... \n",
       "64.367695       1\n",
       "39.233181       1\n",
       "54.232346       1\n",
       "26.891820       1\n",
       "63.649156       1\n",
       "Name: tfidf_score, Length: 5691, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['tfidf_score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tweets['clean_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limpiamos un poco el campo de texto antes de crear los embeddings usando expresiones regulares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def borrar_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)\n",
    "\n",
    "def borrar_html(text):\n",
    "    html = re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "\n",
    "def borrar_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def borrar_puntuacion(text):\n",
    "    table = str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "tweets['clean_text'] = tweets['text'].apply(lambda x : borrar_URL(x))\n",
    "tweets['clean_text'] = tweets['clean_text'].apply(lambda x : borrar_html(x))\n",
    "tweets['clean_text'] = tweets['clean_text'].apply(lambda x: borrar_emojis(x))\n",
    "tweets['clean_text'] = tweets['clean_text'].apply(lambda x : borrar_puntuacion(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a usar Glove para obtener los embeddings pre-entrenados de cada palabra en cada tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, generamos una lista con todas las palabras por cada tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos la función \"word_tokenize\" para obtener las palabras dentro de un tweet. Para homogeneizar dichas palabras, las tomamos en minúsculas y solamente consideramos las palabras que tienen caracteres alfanuméricos, excluyendo a su vez las stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras_por_tweet = []\n",
    "\n",
    "for tweet in tweets['clean_text']:\n",
    "    palabras = [word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in set(stopwords.words('english'))))]\n",
    "    palabras_por_tweet.append(palabras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparemos el resultado del primer tweet con la lista de palabras de ese tweet obtenidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Our Deeds are the Reason of this earthquake May ALLAH Forgive us all'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.iloc[0]['clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Our',\n",
       " 'Deeds',\n",
       " 'are',\n",
       " 'the',\n",
       " 'Reason',\n",
       " 'of',\n",
       " 'this',\n",
       " 'earthquake',\n",
       " 'May',\n",
       " 'ALLAH',\n",
       " 'Forgive',\n",
       " 'us',\n",
       " 'all']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(tweets.iloc[0]['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['our', 'deeds', 'reason', 'earthquake', 'may', 'allah', 'forgive', 'us']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palabras_por_tweet[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, usando la clase Tokenizer de la librería keras, pasamos la lista de palabras por tweet a listas de números enteros de igual longitud (en este caso, listas de 50 dimensiones), completando con 0 las listas con menos de 50 enteros y truncando las listas con más de 50 enteros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(palabras_por_tweet)\n",
    "cantidad_de_palabras = len(tokenizer.word_index) + 1\n",
    "tweets_codificados = tokenizer.texts_to_sequences(palabras_por_tweet)\n",
    "\n",
    "tweets_padded = pad_sequences(tweets_codificados,maxlen=50,truncating='post',padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10876, 50)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_padded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que cada tweet tiene 50 dimensiones. Veamos la lista de números del primer tweet como ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 622, 5461,  738,  175,   80, 1805, 3525,   16,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_padded[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leemos el archivo con los embbedings de cada palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = dict()\n",
    "\n",
    "archivo = open('../Glove/glove.6B.100d.txt','r',encoding='utf8')\n",
    "for linea in archivo:\n",
    "    valores = linea.split()\n",
    "    palabra = valores[0]\n",
    "    vector = np.asarray(valores[1:],'float32')\n",
    "    embeddings[palabra] = vector\n",
    "\n",
    "archivo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el ejemplo de un embedding del archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.093032 ,  1.6301   ,  0.36645  ,  0.29015  , -0.81781  ,\n",
       "       -0.46611  ,  0.037282 ,  0.13208  ,  0.13661  ,  1.2091   ,\n",
       "        0.81466  , -0.12874  ,  0.52387  , -0.024319 ,  0.94072  ,\n",
       "       -0.12509  , -0.34087  ,  0.19321  , -1.0522   ,  0.62915  ,\n",
       "       -0.0699   ,  0.14182  ,  0.49985  ,  0.61688  ,  0.027162 ,\n",
       "        0.22695  , -0.89086  ,  1.4411   , -0.45774  , -0.5922   ,\n",
       "        0.48147  , -0.4387   ,  0.99552  ,  1.3003   , -1.1123   ,\n",
       "       -1.4847   ,  0.53476  , -0.22524  , -0.21366  , -0.14612  ,\n",
       "       -0.50853  ,  0.82284  , -0.075167 , -0.60331  ,  0.86412  ,\n",
       "        0.15815  ,  0.76075  , -0.28173  ,  0.27679  , -0.43729  ,\n",
       "       -0.53438  ,  0.081051 ,  0.98763  , -0.28154  , -0.15944  ,\n",
       "       -1.9562   ,  0.13058  , -0.46506  ,  1.4629   ,  1.262    ,\n",
       "       -0.46154  ,  1.3625   , -0.40815  , -0.24256  ,  0.21117  ,\n",
       "        0.57007  , -1.6566   , -0.21043  ,  0.97449  , -0.16146  ,\n",
       "       -0.33706  ,  0.50539  ,  0.45718  ,  0.2184   ,  0.69558  ,\n",
       "        0.43774  ,  0.4584   ,  0.57097  , -0.17801  ,  0.15078  ,\n",
       "        0.46262  ,  0.46737  , -0.76256  , -0.0044911,  0.146    ,\n",
       "        0.20688  ,  0.10127  ,  0.41124  , -0.76671  , -1.1911   ,\n",
       "        1.6006   , -0.1965   , -0.51986  , -0.1828   , -0.16551  ,\n",
       "        1.0849   , -0.27171  , -0.45208  ,  0.24962  , -0.76453  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings['earthquake']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generamos una matriz con los embeddings de cada palabra que aparece en los tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "matriz_de_embeddings = np.zeros((cantidad_de_palabras,100))\n",
    "\n",
    "for palabra,indice in tokenizer.word_index.items():\n",
    "    if indice > cantidad_de_palabras:\n",
    "        continue\n",
    "    \n",
    "    embedding = embeddings.get(palabra)\n",
    "    \n",
    "    if embedding is not None:\n",
    "        matriz_de_embeddings[indice] = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20272, 100)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matriz_de_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tweets['clean_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descartamos las columnas innecesarias para quedarnos únicamente con los features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = tweets[:len(train_df)].set_index('id').iloc[:,3:]\n",
    "test_features = tweets[len(train_df):].set_index('id').iloc[:,4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos los dataframes de train y test, así como los tweets y la matriz de embeddings, en archivos .csv para usarlos en el Notebook de Algoritmos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.to_csv('../Data/train_features.csv')\n",
    "test_features.to_csv('../Data/test_features.csv')\n",
    "pd.DataFrame(tweets_padded).to_csv('../Data/tweets_padded.csv',index=False)\n",
    "pd.DataFrame(matriz_de_embeddings).to_csv('../Data/matriz_de_embeddings.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Copia de Copia de TP1_Real_or_Not.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
