{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/alepenaa94/TP1_Real_or_Not/blob/master/TP1_Real_or_Not.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iwY7zLPkMCLI"
   },
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook vamos a realizar todo el procesamiento del set de datos, limpieza en los campos de ser necesario y la generacion de los features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "0R_dMyhfL8QX",
    "outputId": "5d011d54-140f-4716-cdd7-7b31c412e540"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KiQERldrjl68"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../Data/train.csv', encoding='latin-1',dtype={'id': np.uint16,'target': np.bool})\n",
    "test_df = pd.read_csv('../Data/test.csv', encoding='latin-1',dtype={'id': np.uint16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "sTfW9uPBjl10",
    "outputId": "237774d0-8fb8-49be-a7a2-ab000ed1c736",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0    True  \n",
       "1    True  \n",
       "2    True  \n",
       "3    True  \n",
       "4    True  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.concat([train_df,test_df])\n",
    "tweets = tweets[['id','keyword','location','text','target']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza de la información"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En base a lo aprendido en el TP1, limpiamos la información de algunos campos del dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['keyword'] = tweets['keyword'].str.replace('%20',' ')\n",
    "tweets['keyword'].fillna('None',inplace=True)\n",
    "tweets['location'].fillna('Unknown',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volvemos a generar las columnas utilizadas para el TP1 ya que podrían resultar features útiles para el modelo de predicción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Cantidad de palabras en el tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['cantidad_de_palabras'] = tweets['text'].apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Longitud del tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['longitud_del_tweet'] = tweets['text'].str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Longitud promedio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['longitud_palabra_promedio'] = tweets['longitud_del_tweet'] / tweets['cantidad_de_palabras']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Cuartiles de longitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>longitud_del_tweet</td>\n",
       "      <td>10876.0</td>\n",
       "      <td>101.790916</td>\n",
       "      <td>34.141486</td>\n",
       "      <td>5.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>169.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      count        mean        std  min   25%    50%    75%  \\\n",
       "longitud_del_tweet  10876.0  101.790916  34.141486  5.0  78.0  108.0  134.0   \n",
       "\n",
       "                      max  \n",
       "longitud_del_tweet  169.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['longitud_del_tweet'].describe().to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.loc[tweets['longitud_del_tweet'] < 78.0,'longitud_categ'] = \"0 a 25\"\n",
    "tweets.loc[tweets['longitud_del_tweet'] >= 78.0,'longitud_categ'] = \"25 a 50\"\n",
    "tweets.loc[tweets['longitud_del_tweet'] >= 108.0,'longitud_categ'] = \"50 a 75\"\n",
    "tweets.loc[tweets['longitud_del_tweet'] >= 134.0,'longitud_categ'] = \"75 a 100\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos el método de Smothing para encodear las variables categoricas. (El código está basado en el obtenido de la siguiente fuente: https://gist.github.com/marnixkoops/e68815d30474786e2b293682ed7cdb01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothing(df, column, target, weight=100):\n",
    "    mean = df[target].mean()\n",
    "    agg = df.groupby(column)[target].agg(['count', 'mean'])\n",
    "\n",
    "    dic = {}\n",
    "    \n",
    "    for i in df[column].unique():\n",
    "        dic[i] = (agg.loc[i]['count'] * agg.loc[i]['mean'] + weight * mean) / (agg.loc[i]['count'] + weight)\n",
    "        \n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tweets[:len(train_df)]\n",
    "df['target'] = df['target'].astype('bool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_categ_encoding_dic = smoothing(df,'longitud_categ','target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['longitud_categ'] = tweets['longitud_categ'].map(long_categ_encoding_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Cantidad de menciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['cantidad_de_menciones'] = tweets['text'].str.count('@')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Tweet expresivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['es_expresivo'] = (tweets['text'].str.contains('\\!\\!') | tweets['text'].str.contains('\\?\\?')).astype('uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Cantidad de hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['cantidad_de_hashtags'] = tweets['text'].str.count('#')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Cantidad de links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['cantidad_de_links'] = tweets['text'].str.count('http')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Cantidad de caracteres no alfanuméricos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos los caracteres no alfanumericos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['cantidad_no_alfanumerico'] = tweets['text'].apply(lambda x: len([char for char in str(x) if char in string.punctuation]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Ubicaciones no alfanuméricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['location_no_alfanumerico'] = (~tweets['location'].str.replace(' ','').str.isalnum()).astype('uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Ubicaciones con tweet único"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def location_unico(location):\n",
    "    ubicaciones_unicas = tweets['location'].value_counts()[tweets['location'].value_counts() == 1].index\n",
    "    return (location in ubicaciones_unicas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['location_unico'] = tweets['location'].map(location_unico).astype('uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Encoding del campo keyword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como la columna keyword es de tipo categórico, vamos a codificarla usando el método de Smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_encoding_dic = smoothing(df,'keyword','target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['keyword_encoded'] = tweets['keyword'].map(keywords_encoding_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\FF633NG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\FF633NG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\FF633NG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#Vamos a limpiar un poco el tweet.\n",
    "pattern_exclude = '(one|dont|cant|would|im|people|go|make|time|love|amp|get|house|update|talk'+\\\n",
    "                  '|want|today|know|say|us|day|crush|see|back|think|look|rigth|remember|car|shes'+\\\n",
    "                  '|thing|let|still|lol|much|thank|take|way|youre|road|another|really|save|hows'+\\\n",
    "                  '|play|even|theres|everyone|feel|year|work|check|two|great|ing|like|sink|href|hr|hs'+\\\n",
    "                  '|every|build|youtuve|video|n|home|body|bag|photo|stay|game|start|gt|fuck|help'+\\\n",
    "                  '|best|well|california|end|live|e|rt|wreck|plan|full|may|ies|u|could|many|last'+\\\n",
    "                  '|find|service|leave|collapse|world|war|destroy|wound|break|right|hear|school)+'\n",
    "\n",
    "def filter_words(tweet):\n",
    "    tweet = re.sub(r'(\\b[\\w]+:\\/\\/[\\w -\\?&;#~=\\.\\/@]+[\\w\\/])', ' ', tweet)\n",
    "    tweet = re.sub(r'\\'', '', tweet)\n",
    "    return re.sub(r'[www.]*[A-z]+.(com|gov|edu|net|mil|org|io|int)+', ' ', tweet)\n",
    "\n",
    "def text_to_blob(tweet):\n",
    "    tweet_blob = TextBlob(str(tweet))\n",
    "    return ' '.join(tweet_blob.words)\n",
    "\n",
    "\n",
    "def normalization(tweet_list):\n",
    "        lem = WordNetLemmatizer()\n",
    "        normalized_tweet = []\n",
    "        for word in tweet_list:\n",
    "            word_aux = word.lower().strip()\n",
    "            if re.match(pattern_exclude,\n",
    "                        word_aux):\n",
    "                continue\n",
    "            normalized_text = lem.lemmatize(word_aux,'v')\n",
    "            normalized_tweet.append(normalized_text)\n",
    "        return normalized_tweet\n",
    "\n",
    "def clean(tweet):\n",
    "    tweet_list = [word for word in (text_to_blob(filter_words(tweet))).split()]\n",
    "    clean_tokens = [tkn for tkn in tweet_list if re.match(r'[A-z]+', tkn)]\n",
    "    clean_s = ' '.join(clean_tokens)\n",
    "    l_aux = normalization(clean_s.split())\n",
    "    return ' '.join([word for word in l_aux if word not in stopwords.words('english')])\n",
    "\n",
    "tweets['clean_text'] = tweets['text'].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = tweets[:len(train_df)]['clean_text'].to_list()\n",
    "\n",
    "tfidf_vectorizer=TfidfVectorizer(analyzer='word', ngram_range=(1,3),sublinear_tf=True, min_df=5, norm='l2',max_features=500, encoding='latin-1', stop_words='english')\n",
    "\n",
    "tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_feature(doc):\n",
    "    #Calcula la similitud entre el doc y lo calculado para todo el set train.\n",
    "    query=TfidfVectorizer(vocabulary=tfidf_vectorizer.vocabulary_)\n",
    "    query=query.fit_transform([doc])\n",
    "    s=pd.Series(linear_kernel(query,tfidf_vectorizer_vectors)[0])\n",
    "    return s.sum()\n",
    "    \n",
    "tweets['tfidf_score']= tweets['clean_text'].apply(similarity_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000000      987\n",
       "57.567868      59\n",
       "77.532286      58\n",
       "106.542617     53\n",
       "78.021807      45\n",
       "             ... \n",
       "64.367695       1\n",
       "39.233181       1\n",
       "54.232346       1\n",
       "26.891820       1\n",
       "63.649156       1\n",
       "Name: tfidf_score, Length: 5691, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['tfidf_score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tweets['clean_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limpiamos un poco el campo de texto antes de crear los embeddings usando expresiones regulares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def borrar_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)\n",
    "\n",
    "def borrar_html(text):\n",
    "    html = re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "\n",
    "def borrar_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def borrar_puntuacion(text):\n",
    "    table = str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "tweets['clean_text'] = tweets['text'].apply(lambda x : borrar_URL(x))\n",
    "tweets['clean_text'] = tweets['clean_text'].apply(lambda x : borrar_html(x))\n",
    "tweets['clean_text'] = tweets['clean_text'].apply(lambda x: borrar_emojis(x))\n",
    "tweets['clean_text'] = tweets['clean_text'].apply(lambda x : borrar_puntuacion(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a usar Glove para obtener los embeddings pre-entrenados de cada palabra en cada tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, generamos una lista con todas las palabras por cada tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos la función \"word_tokenize\" para obtener las palabras dentro de un tweet. Para homogeneizar dichas palabras, las tomamos en minúsculas y solamente consideramos las palabras que tienen caracteres alfabéticos, excluyendo a su vez las stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras_por_tweet = []\n",
    "\n",
    "for tweet in tweets['clean_text']:\n",
    "    palabras = [word.lower() for word in word_tokenize(tweet) if((word.isalpha() == 1) & (word not in set(stopwords.words('english'))))]\n",
    "    palabras_por_tweet.append(palabras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparemos el resultado del primer tweet con la lista de palabras de ese tweet obtenidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Our Deeds are the Reason of this earthquake May ALLAH Forgive us all'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.iloc[0]['clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Our',\n",
       " 'Deeds',\n",
       " 'are',\n",
       " 'the',\n",
       " 'Reason',\n",
       " 'of',\n",
       " 'this',\n",
       " 'earthquake',\n",
       " 'May',\n",
       " 'ALLAH',\n",
       " 'Forgive',\n",
       " 'us',\n",
       " 'all']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(tweets.iloc[0]['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['our', 'deeds', 'reason', 'earthquake', 'may', 'allah', 'forgive', 'us']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palabras_por_tweet[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, usando la clase Tokenizer de la librería keras, pasamos la lista de palabras por tweet a listas de números enteros de igual longitud (en este caso, listas de 50 dimensiones), completando con 0 las listas con menos de 50 enteros y truncando las listas con más de 50 enteros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(palabras_por_tweet)\n",
    "cantidad_de_palabras = len(tokenizer.word_index) + 1\n",
    "tweets_codificados = tokenizer.texts_to_sequences(palabras_por_tweet)\n",
    "\n",
    "tweets_padded = pad_sequences(tweets_codificados,maxlen=50,truncating='post',padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10876, 50)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_padded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que cada tweet tiene 50 dimensiones. Veamos la lista de números del primer tweet como ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 622, 5461,  738,  175,   80, 1805, 3525,   16,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_padded[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leemos el archivo con los embbedings de cada palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = dict()\n",
    "\n",
    "archivo = open('../Glove/glove.6B.300d.txt','r',encoding='utf8')\n",
    "for linea in archivo:\n",
    "    valores = linea.split()\n",
    "    palabra = valores[0]\n",
    "    vector = np.asarray(valores[1:],'float32')\n",
    "    embeddings[palabra] = vector\n",
    "\n",
    "archivo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el ejemplo de un embedding del archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.10902  , -0.086454 , -0.55195  , -0.58138  , -0.063028 ,\n",
       "        0.027674 ,  0.10453  , -0.66351  , -0.23042  , -1.3166   ,\n",
       "        0.62434  ,  1.0765   ,  0.67735  ,  0.14884  ,  0.49519  ,\n",
       "        0.60226  ,  0.27413  , -0.14638  , -0.16783  ,  0.23525  ,\n",
       "        0.56321  ,  0.011081 , -0.36482  , -0.12759  , -0.27178  ,\n",
       "        0.74587  , -0.14232  , -0.084957 ,  0.0054768, -0.10611  ,\n",
       "        0.44424  ,  1.3039   , -0.89714  ,  0.25516  ,  0.44434  ,\n",
       "       -0.01818  ,  0.25995  ,  0.46754  ,  0.70195  , -0.2469   ,\n",
       "        0.92107  ,  0.24039  ,  0.62318  , -0.31352  , -0.16913  ,\n",
       "       -0.0030993,  1.1106   , -0.23675  , -0.16714  ,  0.25508  ,\n",
       "        0.36779  ,  0.38458  ,  0.35204  , -0.59843  , -0.16146  ,\n",
       "        0.52852  ,  0.27234  , -0.048624 , -0.16963  , -0.13194  ,\n",
       "       -0.7143   ,  0.26223  ,  0.23918  , -0.1141   , -0.18335  ,\n",
       "       -0.16616  ,  0.44657  ,  0.13162  , -0.57545  ,  0.12192  ,\n",
       "        0.15368  ,  0.34592  , -0.6688   ,  0.63388  , -0.22247  ,\n",
       "       -0.47362  ,  0.10556  , -0.29105  , -0.83787  ,  0.55167  ,\n",
       "        0.24885  , -0.6368   , -0.79848  , -0.027229 , -0.039982 ,\n",
       "        0.11965  , -0.62348  ,  0.49469  , -0.28581  , -0.14255  ,\n",
       "        0.36722  ,  0.76891  , -0.23521  , -0.21472  , -0.54192  ,\n",
       "        0.0092229,  0.27003  ,  0.12157  ,  0.48323  ,  0.67385  ,\n",
       "        0.85669  ,  0.092207 , -0.66355  ,  0.21769  ,  0.19103  ,\n",
       "       -0.16749  ,  0.19932  , -0.76565  ,  0.49662  ,  0.13814  ,\n",
       "       -0.62734  , -0.72871  , -0.6178   , -0.32182  ,  1.0351   ,\n",
       "        0.19782  ,  0.50589  ,  0.19434  ,  0.37688  ,  0.061146 ,\n",
       "        0.32575  , -0.2545   , -0.55362  ,  0.90465  , -0.13853  ,\n",
       "        0.59316  ,  0.18859  , -0.25705  , -0.27247  , -0.11874  ,\n",
       "       -0.32753  ,  1.0468   ,  0.11421  ,  0.74341  ,  0.47169  ,\n",
       "       -0.51673  , -1.238    ,  0.73322  ,  0.018587 ,  0.056093 ,\n",
       "       -0.45587  ,  0.46865  , -0.28194  ,  0.25764  , -0.043847 ,\n",
       "       -0.37133  ,  1.0693   ,  0.60327  ,  0.34474  , -0.74612  ,\n",
       "        0.62318  , -0.48554  ,  0.31525  ,  0.53021  , -0.078261 ,\n",
       "        0.48932  ,  0.46573  , -0.06956  , -0.27004  , -1.0524   ,\n",
       "       -0.68151  , -0.32986  , -0.34835  , -0.60905  , -0.50247  ,\n",
       "       -0.22335  ,  0.47358  , -0.90991  ,  0.44027  ,  0.4442   ,\n",
       "        0.38949  , -0.41635  ,  0.32241  , -0.77712  ,  0.39311  ,\n",
       "        0.75014  ,  0.75669  ,  0.092978 ,  0.65594  , -0.45188  ,\n",
       "       -0.64512  , -0.4788   ,  0.10037  , -0.015132 ,  0.1679   ,\n",
       "        0.53962  , -0.37114  ,  0.22002  , -0.41767  , -0.51614  ,\n",
       "        0.42848  ,  0.24164  , -0.16677  , -0.23943  ,  0.31074  ,\n",
       "       -0.88514  ,  0.36907  ,  0.11782  , -0.5742   ,  0.94176  ,\n",
       "        0.35079  ,  0.6186   ,  0.60598  ,  0.29029  , -0.01216  ,\n",
       "        0.72622  , -0.01067  ,  0.010598 ,  0.23328  , -0.079795 ,\n",
       "       -0.55835  ,  0.52682  ,  0.074991 , -0.19251  , -0.38348  ,\n",
       "        0.067716 , -0.40263  , -0.24383  , -0.12379  ,  0.60139  ,\n",
       "        0.52412  ,  0.021831 ,  0.068104 , -0.27502  ,  0.47384  ,\n",
       "       -0.15082  ,  0.58026  , -1.0283   , -1.1149   , -0.28339  ,\n",
       "       -0.25772  ,  0.12957  , -0.91176  , -0.048827 ,  0.30634  ,\n",
       "       -0.14892  , -0.12602  ,  0.27485  ,  1.1958   ,  0.51151  ,\n",
       "        0.20976  , -0.01871  , -0.073766 , -0.07194  , -0.57496  ,\n",
       "       -0.084266 , -0.62425  , -0.36057  , -0.28022  ,  0.24199  ,\n",
       "        0.48655  ,  0.6443   ,  0.28809  , -0.60645  , -0.44554  ,\n",
       "        0.43017  ,  0.23354  , -0.053665 ,  0.78628  , -0.10009  ,\n",
       "        0.084107 , -0.29277  ,  0.30225  ,  0.45511  ,  0.072357 ,\n",
       "       -0.42676  ,  0.16377  , -0.4978   ,  0.58351  , -0.73803  ,\n",
       "       -0.28734  , -0.36885  ,  0.07036  , -0.12804  ,  0.069558 ,\n",
       "        0.65038  , -1.4556   , -0.10224  ,  0.31311  ,  0.060896 ,\n",
       "        0.208    ,  0.43907  ,  0.99149  , -0.16269  ,  0.48345  ,\n",
       "        0.44944  , -0.561    , -0.23776  ,  0.31462  , -0.11117  ,\n",
       "       -0.015743 , -0.17506  ,  0.38498  ,  0.031665 , -0.17743  ,\n",
       "        0.98759  ,  0.059789 , -0.046083 ,  0.24762  , -0.11453  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings['earthquake']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generamos una matriz con los embeddings de cada palabra que aparece en los tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "matriz_de_embeddings = np.zeros((cantidad_de_palabras,300))\n",
    "\n",
    "for palabra,indice in tokenizer.word_index.items():\n",
    "    if indice > cantidad_de_palabras:\n",
    "        continue\n",
    "    \n",
    "    embedding = embeddings.get(palabra)\n",
    "    \n",
    "    if embedding is not None:\n",
    "        matriz_de_embeddings[indice] = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20272, 300)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matriz_de_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tweets['clean_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descartamos las columnas innecesarias para quedarnos únicamente con los features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = tweets[:len(train_df)].set_index('id').iloc[:,3:]\n",
    "test_features = tweets[len(train_df):].set_index('id').iloc[:,4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos los dataframes de train y test, así como los tweets y la matriz de embeddings, en archivos .csv para usarlos en el Notebook de Algoritmos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.to_csv('../Data/train_features.csv')\n",
    "test_features.to_csv('../Data/test_features.csv')\n",
    "pd.DataFrame(tweets_padded).to_csv('../Data/tweets_padded.csv',index=False)\n",
    "pd.DataFrame(matriz_de_embeddings).to_csv('../Data/matriz_de_embeddings.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Copia de Copia de TP1_Real_or_Not.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
